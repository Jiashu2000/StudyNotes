{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b1774bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "724967dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/cache/epub/6130/pg6130.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71dc713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = urlopen(url).read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5975c58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1142775"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89924f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg eBook of The Iliad\\r\\n    \\r\\nThis ebook is for the use of anyone anywhere in the'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8e34cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89390f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ff80d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'Iliad', 'This', 'ebook', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "037da1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "069fb736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be8f6eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lives', 'or', 'condition', 'which', 'tradition', 'has', 'handed', 'down', 'to', 'us', ',', 'we', 'must', 'rather', 'consider', 'the', 'general', 'bearing', 'of', 'the', 'whole', 'narrative', ',', 'than', 'the', 'respective', 'probability', 'of', 'its', 'details', '.', 'It', 'is', 'unfortunate', 'for', 'us', ',', 'that', ',', 'of']\n"
     ]
    }
   ],
   "source": [
    "print(text[1020:1060])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7232fe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg™; Paradise Lost; Project Gutenberg; United States;\n",
      "Literary Archive; Gutenberg™ electronic; great Achilles; Gutenberg\n",
      "Literary; thou art; electronic works; blue-eyed maid; Archive\n",
      "Foundation; godlike Hector; native shore; great Ajax; Scæan gate;\n",
      "electronic work; Homeric poems; great Hector; Grecian train\n"
     ]
    }
   ],
   "source": [
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fe49e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.bbc.com/news/world-us-canada-67662871\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39fc39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(url).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "289f671d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html><html><head><meta charSet=\"utf-8\"/><meta name'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html[:60 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "247728c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b246822",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = BeautifulSoup(html).get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c84b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d31891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Claudine', 'Gay', ':', 'Harvard', 'president', 'sorry', 'for', 'remarks', 'on', 'antisemitismHomeNewsSportBusinessInnovationCultureTravelEarthVideoLiveHomeNewsSportBusinessInnovationCultureTravelEarthVideoLiveHomeNewsSportBusinessInnovationCultureTravelEarthVideoLiveClaudine', 'Gay', ':', 'Harvard', 'president', 'sorry', 'for', 'remarks', 'on', 'antisemitismBy', 'Sam']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0782e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39fa180b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter2.ipynb',\n",
       " 'chapter3.ipynb',\n",
       " 'chapter1.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'data']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eacaeddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter some texts: on an exceptionally hot evening early in July\n"
     ]
    }
   ],
   "source": [
    "s = input(\"enter some texts: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a48fd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you typed 8 words.\n"
     ]
    }
   ],
   "source": [
    "print(\"you typed\", len(nltk.word_tokenize(s)), \"words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "43d05481",
   "metadata": {},
   "outputs": [],
   "source": [
    "couplet = \"Rough winds do shake the darling buds of May,\"\\\n",
    "\"And Summer's lease hath all too short a date:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7fb3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "couplet = (\"Rough winds do shake the darling buds of May,\"\n",
    "\"And Summer's lease hath all too short a date:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a338f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rough winds do shake the darling buds of May,And Summer's lease hath all too short a date:\n"
     ]
    }
   ],
   "source": [
    "print(couplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d17ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "couplet = \"\"\"Rough winds do shake the darling buds of May,\n",
    "And Summer's lease hath all too short a date:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b3feafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rough winds do shake the darling buds of May,\n",
      "And Summer's lease hath all too short a date:\n"
     ]
    }
   ],
   "source": [
    "print(couplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03b0c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 6, 5, 4, 3, 2, 1]\n",
    "b = [' ' * 2 * (7 - i) + 'very' * i for i in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3162a41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            very\n",
      "          veryvery\n",
      "        veryveryvery\n",
      "      veryveryveryvery\n",
      "    veryveryveryveryvery\n",
      "  veryveryveryveryveryvery\n",
      "veryveryveryveryveryveryvery\n",
      "  veryveryveryveryveryvery\n",
      "    veryveryveryveryvery\n",
      "      veryveryveryvery\n",
      "        veryveryvery\n",
      "          veryvery\n",
      "            very\n"
     ]
    }
   ],
   "source": [
    "for line in b:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0452de70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "couplet[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8eb5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "s1 = \"A\"\n",
    "print(ord(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d42922ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🐀🐁🐂🐃🐄🐅🐆🐇🐈🐉🐊🐋🐌🐍🐎🐏🐐🐑🐒🐓🐔🐕🐖🐗🐘🐙🐚🐛🐜🐝🐞🐟🐠🐡🐢🐣🐤🐥🐦🐧🐨🐩🐪🐫🐬🐭🐮🐯🐰🐱🐲🐳🐴🐵🐶🐷🐸🐹🐺🐻🐼🐽🐾🐿👀👁👂👃👄👅👆👇👈👉👊👋👌👍👎👏👐👑👒👓👔👕👖👗👘👙👚👛👜👝👞👟👠👡👢👣"
     ]
    }
   ],
   "source": [
    "for i in range(128000, 128100, 1):\n",
    "    print(chr(i), end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d030160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👍\n"
     ]
    }
   ],
   "source": [
    "print(chr(128077))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7884acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👐\n"
     ]
    }
   ],
   "source": [
    "print(chr(128080))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3ee7478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "b'A'\n",
      "1\n",
      "[65]\n"
     ]
    }
   ],
   "source": [
    "print('A')\n",
    "print('A'.encode('utf-8'))\n",
    "print(len('A'.encode('utf-8')))\n",
    "print(list('A'.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4fe882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🥰\n",
      "b'\\xf0\\x9f\\xa5\\xb0'\n",
      "4\n",
      "[240, 159, 165, 176]\n"
     ]
    }
   ],
   "source": [
    "print('🥰')\n",
    "print('🥰'.encode('utf-8'))\n",
    "print(len('🥰'.encode('utf-8')))\n",
    "print(list('🥰'.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffc0c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39e6f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0494af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210687"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5c5a337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abaissed',\n",
       " 'abandoned',\n",
       " 'abased',\n",
       " 'abashed',\n",
       " 'abatised',\n",
       " 'abed',\n",
       " 'aborted',\n",
       " 'abridged',\n",
       " 'abscessed',\n",
       " 'absconded',\n",
       " 'absorbed',\n",
       " 'abstracted',\n",
       " 'abstricted',\n",
       " 'accelerated',\n",
       " 'accepted',\n",
       " 'accidented',\n",
       " 'accoladed',\n",
       " 'accolated',\n",
       " 'accomplished',\n",
       " 'accosted']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# $match the end of the word \n",
    "\n",
    "[w for w in wordlist if re.search('ed$', w)][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e0fc22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abjectly',\n",
       " 'adjuster',\n",
       " 'dejected',\n",
       " 'dejectly',\n",
       " 'injector',\n",
       " 'majestic',\n",
       " 'objectee',\n",
       " 'objector',\n",
       " 'rejecter',\n",
       " 'rejector',\n",
       " 'unjilted',\n",
       " 'unjolted',\n",
       " 'unjustly']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# . wildcard matches any single character\n",
    "\n",
    "[w for w in wordlist if re.search('^..j..t..$', w)][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d065b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i sent you an email , did you get the e-mail ?\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53e10c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['email', 'e-mail']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ? symbol specifies that the previous character is optional\n",
    "\n",
    "[w for w in text if re.search('^e-?mail$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a234bc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gold', 'golf', 'hold', 'hole']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c834dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa6e041d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6066"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chat_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26a89b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'aaaaaaaaaaaaaaaaa', 'aaahhhh', 'ah', 'ahah', 'ahahah', 'ahh', 'ahhahahaha', 'ahhh', 'ahhhh', 'ahhhhhh', 'ahhhhhhhhhhhhhh', 'h', 'ha', 'haaa', 'hah', 'haha', 'hahaaa', 'hahah', 'hahaha', 'hahahaa', 'hahahah', 'hahahaha', 'hahahahaaa', 'hahahahahaha', 'hahahahahahaha', 'hahahahahahahahahahahahahahahaha', 'hahahhahah', 'hahhahahaha']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in chat_words if re.search('^[ha]+$', w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de7a38fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee',\n",
       " 'miiiiiinnnnnnnnnneeeeeeee',\n",
       " 'mine',\n",
       " 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in chat_words if re.search('^m+i+n+e+$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "169d0fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '!!', '!!!', '!!!!', '!!!!!', '!!!!!!', '!!!!!!!', '!!!!!!!!', '!!!!!!!!!', '!!!!!!!!!!', '!!!!!!!!!!!', '!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!.', '!!!!!.']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in chat_words if re.search('[^aeiouAEIOU]+$', w)][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b26ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "58f4920f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.1', '0.2', '0.3', '0.4', '0.5', '0.7', '0.9', '1.1', '1.2', '1.4']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[0-9]+\\.+[0-9]$', w)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e88af87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C$', 'US$']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[A-Z]+\\$$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a258d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1614',\n",
       " '1637',\n",
       " '1787',\n",
       " '1901',\n",
       " '1903',\n",
       " '1917',\n",
       " '1925',\n",
       " '1929',\n",
       " '1933',\n",
       " '1934']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[0-9]{4}$', w)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aa3c4896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10-day',\n",
       " '10-lap',\n",
       " '10-year',\n",
       " '100-share',\n",
       " '12-point',\n",
       " '12-year',\n",
       " '14-hour',\n",
       " '15-day',\n",
       " '150-point',\n",
       " '190-point']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0d2ebdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['black-and-white',\n",
       " 'bread-and-butter',\n",
       " 'father-in-law',\n",
       " 'machine-gun-toting',\n",
       " 'savings-and-loan']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e46c5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['62%-owned',\n",
       " 'Absorbed',\n",
       " 'According',\n",
       " 'Adopting',\n",
       " 'Advanced',\n",
       " 'Advancing',\n",
       " 'Alfred',\n",
       " 'Allied',\n",
       " 'Annualized',\n",
       " 'Anything']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('(ed|ing)$', w)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f16afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "238332e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'e', 'i', 'o', 'u']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[aeiou]', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c8dc2e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'io': 549, 'ea': 476, 'ie': 331, 'ou': 329, 'ai': 261, 'ia': 253, 'ee': 217, 'oo': 174, 'ua': 109, 'au': 106, ...})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(vs for w in wsj \n",
    "              for vs in re.findall(r'[aeiou]{2,}', w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f8848c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "406c3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(word):\n",
    "    pieces = re.findall(regexp, word)\n",
    "    return ''.join(pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "95327955",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_udhr = nltk.corpus.udhr.words(\"English-Latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8bc4cf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\n",
      "of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\n",
      "of frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmp\n"
     ]
    }
   ],
   "source": [
    "print(nltk.tokenwrap(compress(w) for w in english_udhr)[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "03e59708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "57d5eac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ing']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e5e2e12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processing']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "94ee77d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('process', 'ing')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b8c63595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('processe', 's')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8fe82ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('process', 'es')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# *? non-greedy match\n",
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c085fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg, nps_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ca910ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c9b8d1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n"
     ]
    }
   ],
   "source": [
    "moby.findall(r\"<a>(<.*>)<man>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d1486080",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = nltk.Text(nps_chat.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "509b2122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you rule bro; telling you bro; u twizted bro\n"
     ]
    }
   ],
   "source": [
    "chat.findall(r\"<.*><.*><bro>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "64a37d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\n",
      "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n"
     ]
    }
   ],
   "source": [
    "chat.findall(r\"<l.*>{3,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8437ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bae66e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "hobbies_learned = nltk.Text(brown.words(categories = ['hobbies', 'learned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5c9650f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed and other activities; water and other liquids; tomb and other\n",
      "landmarks; Statues and other monuments; pearls and other jewels;\n",
      "charts and other items; roads and other features; figures and other\n",
      "objects; military and other areas; demands and other factors;\n",
      "abstracts and other compilations; iron and other metals\n"
     ]
    }
   ],
   "source": [
    "hobbies_learned.findall(r\"<\\w*><and><other><\\w*s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4abc95cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    " is no basis for a system of government. Supreme executive power derives from\n",
    " a mandate from the masses, not from some farcical aquatic ceremony.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "25ed460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "23597197",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fa269e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a9ef0560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n"
     ]
    }
   ],
   "source": [
    "print([porter.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3fb5a48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5576ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = nltk.Index((porter.stem(word), i) for (i, word) in enumerate(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c9bf4132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'mandat', 'the', 'mass', 'not', 'some', 'farcic', 'aquat', 'ceremoni'])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e661f05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('denni', [0]), (':', [1]), ('listen', [2]), (',', [3, 30]), ('strang', [4]), ('women', [5]), ('lie', [6]), ('in', [7]), ('pond', [8]), ('distribut', [9]), ('sword', [10]), ('is', [11]), ('no', [12]), ('basi', [13]), ('for', [14]), ('a', [15, 25]), ('system', [16]), ('of', [17]), ('govern', [18]), ('.', [19, 37]), ('suprem', [20]), ('execut', [21]), ('power', [22]), ('deriv', [23]), ('from', [24, 27, 32]), ('mandat', [26]), ('the', [28]), ('mass', [29]), ('not', [31]), ('some', [33]), ('farcic', [34]), ('aquat', [35]), ('ceremoni', [36])])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "08bb7310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   dog'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%6s\" %'dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d5a4949a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog   '"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%-6s\" %'dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "adcf2f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count, total = 3205, 9375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "46d19ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuray for 9375 is 34.1867%'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"accuray for %d is %2.4f%%\" %(total, 100 * count/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0323a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd= nltk.ConditionalFreqDist(\n",
    "    (genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories = genre)\n",
    ")\n",
    "\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0faf9f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabulate(cfddist, words, categories):\n",
    "    print(\"%-16s\" % \"Category\", end = \" \")\n",
    "    for word in words:\n",
    "        print(\"%6s\" % word, end = \" \")\n",
    "    print()\n",
    "    \n",
    "    for category in categories:\n",
    "        print(\"%-16s\" %category, end = \" \")\n",
    "        for word in words:\n",
    "            print(\"%6d\" %cfddist[category][word], end = \" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8deee36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category            can  could    may  might   must   will \n",
      "news                 93     86     66     38     50    389 \n",
      "religion             82     59     78     12     54     71 \n",
      "hobbies             268     58    131     22     83    264 \n",
      "science_fiction      16     49      4     12      8     16 \n",
      "romance              74    193     11     51     45     43 \n",
      "humor                16     30      8      8      9     13 \n"
     ]
    }
   ],
   "source": [
    "tabulate(cfd, modals, genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "766eafe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   can'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%6s\" % \"can\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "892b9147",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"data/output.txt\", 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f10b5b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.genesis.words('english-kjv.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d83112b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    output_file.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b5dc4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1164a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "saying = ['After', 'all', 'is', 'said', 'and', 'done', ',',\n",
    "          'more', 'is', 'said', 'than', 'done', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f49322ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After(5) all(3) is(2) said(4) and(3) done(4) ,(1) more(4) is(2) said(4) than(4) done(4) .(1) "
     ]
    }
   ],
   "source": [
    "for word in saying:\n",
    "    print(word + \"(\" + str(len(word)) + \")\", end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e1c20a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7a47021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "format = '%s_(%d),'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2c818be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = [format %(word, len(word)) for word in saying]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "97e7723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ' '.join(pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a85ad856",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped = fill(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "77ab60e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After (5), all (3), is (2), said (4), and (3), done (4), , (1),\n",
      "more (4), is (2), said (4), than (4), done (4), . (1),\n"
     ]
    }
   ],
   "source": [
    "print(wrapped.replace(\"_\", ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e704b",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9703fc7b",
   "metadata": {},
   "source": [
    "1. Define a string s = 'colorless'. Write a Python statement that changes this to “colourless” using only the slice and concatenation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d1be710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'colorless'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a6000502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colourless'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:4] + 'u' + s[4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56855021",
   "metadata": {},
   "source": [
    "2. We can use the slice notation to remove morphological endings on words. For example, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice notation to remove the affixes from these words (we’ve inserted a hyphen to indicate the affix boundary, but omit this from your strings): dish-es, run-ning, nationality, un-do, pre-heat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "64f11fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "affixed = [('dishes', 2), \n",
    "           ('running', 4),\n",
    "           ('nationality', 5),\n",
    "           ('undo', 2),\n",
    "           ('preheat', 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f91df48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dish\n",
      "run\n",
      "nation\n",
      "un\n",
      "pre\n"
     ]
    }
   ],
   "source": [
    "for word, pos in affixed:\n",
    "    print(word[:-pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e55d933",
   "metadata": {},
   "source": [
    "3.We saw how we can generate an IndexError by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "14585bf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[182], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhello\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "'hello'[-7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dba707c",
   "metadata": {},
   "source": [
    "4. We can specify a “step” size for the slice. The following returns every second character within the slice: monty[6:11:2]. It also works in the reverse direction: monty[10:5:-2]. Try these for yourself, and then experiment with different step values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f37e1aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = \"hello. it is a  good day today. i will success!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c46e63f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l.ts'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[2: 12: 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd4225",
   "metadata": {},
   "source": [
    "5. What happens if you ask the interpreter to evaluate monty[::-1]? Explain why this is a reasonable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "133378c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!sseccus lliw i .yadot yad doog  a si ti .olleh'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f4e5c",
   "metadata": {},
   "source": [
    "6. Describe the class of strings matched by the following regular expressions:\n",
    "    a. [a-zA-Z]+\n",
    "    b. [A-Z][a-z]*\n",
    "    c. p[aeiou]{,2}t\n",
    "    d. \\d+(\\.\\d+)?\n",
    "    e. ([^aeiou][aeiou][^aeiou])*\n",
    "    f. \\w+|[^\\w\\s]+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d89b840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{cAMELCASE} 6186258313 {hybr}1{d}\n"
     ]
    }
   ],
   "source": [
    "# a. [a-zA-Z]+\n",
    "\n",
    "# one or more alphabets\n",
    "\n",
    "nltk.re_show(\"[a-zA-Z]+\", \"cAMELCASE 6186258313 hybr1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b8077c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{I} think words beginning with {Uppercase} {Letters} will be matched, or any uppercase letters found in o{T}{H}{E}{R} positions.\n"
     ]
    }
   ],
   "source": [
    "# b. [A-Z][a-z]*\n",
    "\n",
    "# uppercase alphabet + zero or more lowercase alphabet\n",
    "\n",
    "test = 'I think words beginning with Uppercase Letters will be matched, ' \\\n",
    "       'or any uppercase letters found in oTHER positions.'\n",
    "\n",
    "nltk.re_show(\"[A-Z][a-z]*\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "1422c8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abaptiston',\n",
       " 'abepithymia',\n",
       " 'ableptical',\n",
       " 'ableptically',\n",
       " 'abrupt',\n",
       " 'abruptedly',\n",
       " 'abruption',\n",
       " 'abruptly',\n",
       " 'abruptness',\n",
       " 'absorpt',\n",
       " 'absorptance',\n",
       " 'absorptiometer',\n",
       " 'absorptiometric',\n",
       " 'absorption',\n",
       " 'absorptive',\n",
       " 'absorptively',\n",
       " 'absorptiveness',\n",
       " 'absorptivity',\n",
       " 'absumption',\n",
       " 'acalyptrate']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c. p[aeiou]{,2}t\n",
    "\n",
    "# with p, 0-2 vowels in the middle, then with t\n",
    "\n",
    "[w for w in wordlist if re.search(\"p[aeiou]{,2}t\", w)][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "622499c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1234', '12.34', 'example 123.4 in a string', '1-234', '12,4', '$12.34']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d. \\d+(\\.\\d+)?\n",
    "\n",
    "# numbers, with or without digits\n",
    "\n",
    "test = ['1234', '12.34', 'example 123.4 in a string', '1-234', '12,4', '$12.34']\n",
    "\n",
    "[w for w in test if re.search(r\"\\d+(\\.\\d+)?\", w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6679fb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.23}.{4}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r\"\\d+(\\.\\d+)?\", \"1.23.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "fe17a990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{babbabbabbab}{}a{pap}{}a{}\n"
     ]
    }
   ],
   "source": [
    "# e. ([^aeiou][aeiou][^aeiou])*\n",
    "\n",
    "# (non-vowel + vowel + non-vowel) repeat\n",
    "\n",
    "string = \"babbabbab\" \\\n",
    "         \"babapapa\"\n",
    "nltk.re_show(r'([^aeiou][aeiou][^aeiou])*', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b1bd793f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{This} {RegExp} {needs} {a} {fairly} {long} {string} {to} {show} {what} {it} {can} {%#$^} {%&*} {do}{.}\n"
     ]
    }
   ],
   "source": [
    "# f. \\w+|[^\\w\\s]+\n",
    "\n",
    "# alphabets or non-alphabet and non-space\n",
    "\n",
    "string = \"This RegExp needs a fairly long string to show what it can %#$^ %&* do.\"\n",
    "nltk.re_show(r'\\w+|[^\\w\\s]+', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5601561",
   "metadata": {},
   "source": [
    "7. Write regular expressions to match the following classes of strings:\n",
    "    a. A single determiner (assume that a, an, and the are the only determiners)\n",
    "    b. An arithmetic expression using integers, addition, and multiplication, such as\n",
    "    2*3+8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6aa470c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think {a} relevant string like {the} one here is {an} example of what we need.\n"
     ]
    }
   ],
   "source": [
    "reg = r\"\\b[Aa]n?\\b|\\b[Tt]he\\b\"\n",
    "\n",
    "string = \"I think a relevant string like the one here is an example of what we need.\"\n",
    "\n",
    "nltk.re_show(reg, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "72329251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2 * 3 + 8}\n"
     ]
    }
   ],
   "source": [
    "reg = r\"(\\d|[*+= ])+\"\n",
    "\n",
    "string = \"2 * 3 + 8\"\n",
    "\n",
    "nltk.re_show(reg, string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba0e024",
   "metadata": {},
   "source": [
    "8. Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use urllib.urlopen to access the contents of the URL, e.g.: raw_contents = urllib.urlopen('http://www.nltk.org/').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b12293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7037f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_contents = urllib.request.urlopen('http://www.nltk.org/').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28797c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd847c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_url_contents(url):\n",
    "    html = urllib.request.urlopen(url).read().decode('utf8')\n",
    "    raw = BeautifulSoup(html, 'html.parser')\n",
    "    for r in raw(['script', 'style']):\n",
    "        r.extract()\n",
    "    \n",
    "    text = ' '.join(raw.stripped_strings)\n",
    "    return normalize('NFKD', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5bf2067d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Laura Kuenssberg: Ukraine in 'mortal danger' without aid, Olena Zelenska warns Home News Sport Business Innovation Culture Travel Earth Video Live Home News Sport Business Innovation Culture Travel Ea\""
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.bbc.com/news/world-europe-67667035\"\n",
    "\n",
    "return_url_contents(url)[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd36481",
   "metadata": {},
   "source": [
    "9.Save some text into a file corpus.txt. Define a function load(f) that reads from the file named in its sole argument, and returns a string containing the text of the file.\n",
    "    \n",
    "    a. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multiline regular expression inline comments, using the verbose flag (?x).\n",
    "    \n",
    "    b. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expressions: monetary amounts; dates; names of people and organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2e8293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ceecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc3f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c5f9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jiashu/Documents/StudyNotes/nlp/data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "916fefec",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.bbc.com/news/world-middle-east-67670679'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d88a889",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = return_url_contents(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28b80ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Israel-Gaza war: Half of Gaza's population is starving, warns UN Home News Sport Business Innovation Culture Travel Earth Video Live Home News Sport Business Innovation Culture Travel Earth Video Live\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be532b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus.txt', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1ea2c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(f):\n",
    "    text = open(f, encoding = 'utf-8')\n",
    "    raw = text.read()\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "279b8daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc = load(\"corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7d10601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':', \"'\", ',', ':', \"'\", ',', \"'\", ',', '.', ',', ',', '.', '\"', '\"', ',', '.', '.', '\"', ',', \"'\", '\"', '.', '\"', ',', '\"', '.', ',', \"'\", ',', '.', ',', ',', '.', ',', ',', ',', '.', ',', '.', '.', '.', '\"', ',', ',', '\"', '.', '\"', ',', ',', ',', ',', '\"', '.', ',', '.', '\"', '\"', ',', '.', ',', ',', '.', ',', \"'\", ',', ',', '.', '\"', ',', ',', '(', ')', ',', ',', '.', \"'\", '.', ',', '\"', '.', '\"', ',', ',', ',', '?', ',', ',', '.', '\"', \"'\", ',', ',', '\"', '\"', '.', ',', ',', '\"', '\"', \"'\", '.', ':', ':', ':', ':', ':', \"'\", \"'\", \"'\", ':', '?', ':', ',', ',', ',', ',', '.', ',', '.', '.', ',', ',', '\"', ',', ',', '[', ']', '\"', '.', ',', ',', ',', '\"', '\"', '\"', '\"', '.', \"'\", ',', ',', '\"', '\"', '.', '.', ',', '.', '.', ',', ',', ',', ',', \"'\", '.', '.', '.', '?', ':', ',', '.', '.', '.', '.', '.', ',', ',', \"'\", '.', '.', '.', '.', '.', ',', ',', ',', ',', '.', ',', '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)\n",
    "    [][.,:;\"'?!():_-`]\n",
    "'''\n",
    "\n",
    "print(nltk.regexp_tokenize(bbc, pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26269b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Home News Sport Business Innovation Culture Travel Earth Video Live Home News Sport Business Innovation Culture Travel Earth Video Live Home News Sport Business Innovation Culture Travel Earth Video Live Israel', 'By Fiona Nimoni', 'News Reuters Young', 'Khan Younis', 'Carl Skau', 'World Food Programme', '10', 'Mr Skau', 'Israel Defence Forces', 'Lt Col Richard Hecht', 'Gaza Strip', '7', '1,200', '240', 'The Hamas', '17,700', '7,000', 'Kerem Shalom', 'Mr Skau', 'Gaza Strip', '10', 'Mr Skau', 'Khan Younis', 'Dr Ahmed Moghrabi', 'Dr Ahmed Moghrabi', 'Khan Younis', 'Khan Younis', 'Jeremy Bowen', 'The Israeli', 'Palestinians On', 'The Israel', 'Mahmoud Abbas', 'United States', 'Security Council', '15', 'Security Council', '13', 'Mr Abbas', 'Palestinian Authority', 'Robert Wood', '7', 'Prime Minister', 'Benjamin Netanyahu', '78', '180', '100', 'On Saturday', 'Sahar Baruch', '25', 'Palestinians Israel', 'Israel Israel', 'Related The', '15', 'Middle East Why', '1', 'Middle East Gaza Strip', '2', 'Middle East More', '12', '12', 'Middle East', '12', 'Israel It', '100', '7', '12', 'Middle East', '14', 'Gaza Islam Alashi', '14', '14', 'Gaza Islam Alashi', '14', '15', 'Jeremy Bowen', '15', 'Middle East Home News Sport Business Innovation Culture Travel Earth Video Live', 'Use About', 'Privacy Policy Cookies Accessibility Help Contact', 'Do Not Sell My Info Copyright', '2023', 'C. All', 'Beta Terms By', 'Beta Site', 'Beta Site', 'Beta Site', 'Beta Terms', 'Beta Site', 'The Beta Site', 'Beta Terms', 'Beta Terms', 'Home News News Israel', 'Gaza War War', 'Ukraine World Africa Asia Australia Europe Latin America Middle East', 'England N. Ireland Scotland Wales In Pictures', 'Verify Sport Business Business Future', 'Business Technology', 'Business Work Culture Market Data Innovation Innovation Technology Science', 'Health Artificial Intelligence Culture Culture Film', 'Music Art', 'Design Style Books Entertainment News Travel Travel Destinations World', 'Table Culture', 'Experiences Adventures The Specia', 'List Earth Earth Natural Wonders Weather', 'Science Climate Solutions Sustainable Business Green Living Video Live Live Live News Live Sport Audio']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)\n",
    "          \n",
    "          (?:[A-Z])(?:[a-z]+|\\.)(?:\\s+[A-Z](?:[a-z]+|\\.))*(?:\\s+[A-Z])(?:[a-z]+|\\.)\n",
    "                                         # proper names\n",
    "          | \\$\\d+\\s\\b[tr|b|m]illion\\b    # literal monetary amounts\n",
    "          | \\$?\\d+(?:[,\\.]\\d+)?          # numerical monetary amounts\n",
    "          | \\d{2}\\[\\\\]\\d{2}\\-\\\\]\\d{4}    # numerical dates (U.S. format)\n",
    "          | [A-Z][a-z.]*\\s\\d{2}\\,\\s\\d{2, 4} # literal dates (U.S. format)\n",
    "\n",
    "          \n",
    "        '''\n",
    "\n",
    "print(nltk.regexp_tokenize(bbc, pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d89f0",
   "metadata": {},
   "source": [
    "10. Rewrite the following loop as a list comprehension: \n",
    "\n",
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "\n",
    "result = []\n",
    "\n",
    "for word in sent:\n",
    "\n",
    "    word_len = (word, len(word))\n",
    "    result.append(word_len)\n",
    "    \n",
    "result\n",
    "\n",
    "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff66b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b807514b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(The, 3) (dog, 3) (gave, 4) (John, 4) (the, 3) (newspaper, 9) "
     ]
    }
   ],
   "source": [
    "format = \"(%s, %d)\"\n",
    "\n",
    "for w in sent:\n",
    "    print(format%(w,len(w)), end = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda2565",
   "metadata": {},
   "source": [
    "11. Define a string raw containing a sentence of your own choosing. Now, split raw on some character other than space, such as 's'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55001821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How much ', ' would a ', 'chuck chuck if a ', 'chuck could chuck ', '?']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\"\n",
    "raw.split('wood')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b2375",
   "metadata": {},
   "source": [
    "12. Write a for loop to print out the characters of a string, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "301c0606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "o\n",
      "w\n",
      " \n",
      "m\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "w\n"
     ]
    }
   ],
   "source": [
    "for i in raw[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a523b848",
   "metadata": {},
   "source": [
    "13. What is the difference between calling split on a string with no argument and one with ' ' as the argument, e.g., sent.split() versus sent.split(' ')? What happens when the string being split contains tab characters, consecutive space characters, or a sequence of tabs and spaces? (In IDLE you will need to use '\\t' to enter a tab character.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa8bf167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'is', 'a', 'pretty', 'simple', 'string.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This', 'string', 'is', 'a', 'pretty', 'simple', 'string.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'strings', 'has', 'tabs.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This\\tstrings\\thas\\ttabs.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'has', 'lots', 'of', 'space.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This', '', '', '', '', '', '', '', 'string', '', '', '', '', '', '', '', '', '', 'has', '', '', '', '', '', 'lots', '', '', '', 'of', '', '', '', '', 'space.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'has', 'tabs', 'and', 'spaces.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This\\tstring', '', '', '', '', '', '', '', '', 'has\\ttabs', '', '', '', '', '', '', 'and\\tspaces.']\n"
     ]
    }
   ],
   "source": [
    "# sent.split() splits all whitespace equally\n",
    "# sent.split('') splits all whitespace literally\n",
    "\n",
    "s1 = \"This string is a pretty simple string.\"\n",
    "s2 = \"This\\tstrings\\thas\\ttabs.\"\n",
    "s3 = \"This        string          has      lots    of     space.\"\n",
    "s4 = \"This\\tstring         has\\ttabs       and\\tspaces.\"\n",
    "\n",
    "Ss = [s1, s2, s3, s4]\n",
    "\n",
    "for s in Ss:\n",
    "    print(\"\\nWith `sent.split()`:\")\n",
    "    print(s.split())\n",
    "    print(\"\\nWith `sent.split(' ')`:\")\n",
    "    print(s.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89778845",
   "metadata": {},
   "source": [
    "14. Create a variable words containing a list of words. Experiment with words.sort() and sorted(words). What is the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90e2c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['helli', 'ercmc', 'mmod', 'cigh', 'kaka', 'kitty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ba2f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f991a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cigh', 'ercmc', 'helli', 'kaka', 'kitty', 'mmod']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bb477e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['helli', 'ercmc', 'mmod', 'cigh', 'kaka', 'kitty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d45da6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cigh', 'ercmc', 'helli', 'kaka', 'kitty', 'mmod']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b53d388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['helli', 'ercmc', 'mmod', 'cigh', 'kaka', 'kitty']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f0cfc",
   "metadata": {},
   "source": [
    "##### words.sort() does not return a value, but it alters the ordering of the list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f044e472",
   "metadata": {},
   "source": [
    "15. Explore the difference between strings and integers by typing the following at a Python prompt: \"3\" * 7 and 3 * 7. Try converting between strings and integers using int(\"3\") and str(3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8792d2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"3\" * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a2b9da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(\"3\") * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7291cd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(3) * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a10abae",
   "metadata": {},
   "source": [
    "16. Earlier, we asked you to use a text editor to create a file called test.py, containing the single line monty = 'Monty Python'. If you haven’t already done this (or can’t find the file), go ahead and do it now. Next, start up a new session with the Python interpreter, and enter the expression monty at the prompt. You will get an error from the interpreter. Now, try the following (note that you have to leave off the .py part of the filename):\n",
    "\n",
    "from test import msg\n",
    "\n",
    "msg\n",
    "\n",
    "This time, Python should return with a value. You can also try import test, in\n",
    "which case Python should be able to evaluate the expression test.monty at the\n",
    "prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "270b1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import test121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10526f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jiashu/Documents/StudyNotes/nlp/data\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3183fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test121 import monty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24925a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'monty python'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test121.monty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11593167",
   "metadata": {},
   "source": [
    "17. What happens when the formatting strings %6s and %-6s are used to display strings that are longer than six characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce3a6386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"hello world\"\n",
    "\n",
    "\"%6s\"%s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5548e4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%-6s\"%s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6f978a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%.6s\"%s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df06d0",
   "metadata": {},
   "source": [
    "18. Read in some text from a corpus, tokenize it, and print the list of all wh-word\n",
    "types that occur. (wh-words in English are used in questions, relative clauses, and\n",
    "exclamations: who, which, what, and so on.) Print them in order. Are any words\n",
    "duplicated in this list, because of the presence of case distinctions or punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9d49d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8839e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = gutenberg.raw(\"bryant-stories.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0607d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "tokens = sorted(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e08a720a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Whale', 'What', 'When', 'Whenever', 'Where', 'Whether', 'Whiff', 'While', 'Whirling', 'White', 'Who', 'Whose', 'Why', 'what', 'whatever', 'wheat', 'wheelbarrow', 'wheeled', 'when', 'whence', 'whenever', 'where', 'wherein', 'wherever', 'whether', 'which', 'while', 'whimpering', 'whin', 'whinny', 'whipped', 'whirlpool', 'whiruled', 'whisk', 'whisked', 'whisper', 'whisper_', 'whispered', 'whispering', 'whispers', 'whistle', 'whistled', 'white', 'white-haired', 'white-robed', 'whither', 'who', 'whole', 'wholly', 'whom', 'whose', 'why']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in tokens if re.search('^[Ww]h', w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e2f833",
   "metadata": {},
   "source": [
    "19.Create a file consisting of words and (made up) frequencies, where each line\n",
    "consists of a word, the space character, and a positive integer, e.g., fuzzy 53. Read\n",
    "the file into a Python list using open(filename).readlines(). Next, break each line\n",
    "into its two fields using split(), and convert the number into an integer using\n",
    "int(). The result should be a list of the form: [['fuzzy', 53], ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38fb4ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jiashu/Documents/StudyNotes/nlp/data'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8bcccc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'corpus.txt',\n",
       " 'alice.txt',\n",
       " '__pycache__',\n",
       " 'wordcount.txt',\n",
       " 'test121.py',\n",
       " 'output.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc482aad",
   "metadata": {},
   "source": [
    "20. Write code to access a favorite web page and extract some text from it. For\n",
    "example, access a weather site and extract the forecast top temperature for your\n",
    "town or city today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "251d65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c06e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '3a4ce04e80f46e8a9e70379690a56184'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f01c4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = \"Seattle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7264323",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "356212cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6ed3320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ad1045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3f3245a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coord': {'lon': -122.3321, 'lat': 47.6062},\n",
       " 'weather': [{'id': 701,\n",
       "   'main': 'Mist',\n",
       "   'description': 'mist',\n",
       "   'icon': '50n'}],\n",
       " 'base': 'stations',\n",
       " 'main': {'temp': 278.07,\n",
       "  'feels_like': 276.19,\n",
       "  'temp_min': 276.88,\n",
       "  'temp_max': 279.17,\n",
       "  'pressure': 1019,\n",
       "  'humidity': 94},\n",
       " 'visibility': 2414,\n",
       " 'wind': {'speed': 2.24, 'deg': 56, 'gust': 4.02},\n",
       " 'clouds': {'all': 100},\n",
       " 'dt': 1702182677,\n",
       " 'sys': {'type': 2,\n",
       "  'id': 131480,\n",
       "  'country': 'US',\n",
       "  'sunrise': 1702136735,\n",
       "  'sunset': 1702167479},\n",
       " 'timezone': -28800,\n",
       " 'id': 5809844,\n",
       " 'name': 'Seattle',\n",
       " 'cod': 200}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a2a3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_to_c(temp):\n",
    "    return temp - 273.15\n",
    "\n",
    "def k_to_f(temp):\n",
    "    return (temp - 273.15) * 1.8 + 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b542bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temp(city):\n",
    "    url = f'http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        current_k = data['main']['temp']\n",
    "        min_k = data['main']['temp_min']\n",
    "        max_k = data['main']['temp_max']\n",
    "        print(\"the current temp is {:.1f}℃ / {:.1f}℉\".format(k_to_c(current_k), k_to_f(current_k)))\n",
    "        print(\"Today's high temp is {:.1f}℃ / {:.1f}℉\".format(k_to_c(max_k), k_to_f(max_k)))\n",
    "        print(\"Today's low temp is {:.1f}℃ / {:.1f}℉\".format(k_to_c(min_k), k_to_f(min_k)))\n",
    "    else:\n",
    "        print('cannot get temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9bf45d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current temp is 4.9℃ / 40.9℉\n",
      "Today's high temp is 6.0℃ / 42.8℉\n",
      "Today's low temp is 3.7℃ / 38.7℉\n"
     ]
    }
   ],
   "source": [
    "get_temp(\"Seattle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "89844989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current temp is 13.9℃ / 57.1℉\n",
      "Today's high temp is 13.9℃ / 57.1℉\n",
      "Today's low temp is 13.6℃ / 56.4℉\n"
     ]
    }
   ],
   "source": [
    "get_temp(\"Chengdu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d36bad",
   "metadata": {},
   "source": [
    "21. Write a function unknown() that takes a URL as its argument, and returns a list\n",
    "of unknown words that occur on that web page. In order to do this, extract all\n",
    "substrings consisting of lowercase letters (using re.findall()) and remove any\n",
    "items from this set that occur in the Words Corpus (nltk.corpus.words). Try to\n",
    "categorize these words manually and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "060f48ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = [w.lower() for w in nltk.corpus.words.words('en')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bcd3a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = ['ate', 'beat', 'beaten', 'became', 'become', 'began', 'begun', 'bent', \n",
    "         'bet', 'bid', 'bit', 'bitten', 'blew', 'blown', 'bought', 'broke', 'broken', \n",
    "         'brought', 'built', 'burnt', 'came', 'caught', 'chose', 'chosen', 'come', \n",
    "         'cost', 'cut', 'did', 'dived', 'done', 'dove', 'drank', 'drawn', 'dreamt', \n",
    "         'drew', 'driven', 'drove', 'drunk', 'dug', 'eaten', 'fallen', 'fell', 'felt', \n",
    "         'flew', 'flown', 'forgave', 'forgiven', 'forgot', 'forgotten', 'fought', 'found', \n",
    "         'froze', 'frozen', 'gave', 'given', 'gone', 'got', 'gotten', 'grew', 'grown', \n",
    "         'had', 'heard', 'held', 'hid', 'hidden', 'hit', 'hung', 'hurt', 'kept', 'knew', \n",
    "         'known', 'laid', 'lain', 'lay', 'led', 'left', 'lent', 'let', 'lost', 'made', \n",
    "         'meant', 'met', 'paid', 'put', 'ran', 'rang', 'read', 'ridden', 'risen', 'rode', \n",
    "         'rose', 'run', 'rung', 'said', 'sang', 'sat', 'saw', 'seen', 'sent', 'showed', \n",
    "         'shown', 'shut', 'slept', 'sold', 'spent', 'spoke', 'spoken', 'stood', 'sung', \n",
    "         'swam', 'swum', 'taken', 'taught', 'thought', 'threw', 'thrown', 'told', 'took', \n",
    "         'tore', 'torn', 'understood', 'went', 'woke', 'woken', 'won', 'wore', 'worn', \n",
    "         'written', 'wrote']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f1b883d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist += verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd49784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8e9284f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown(url, es = False, s = False, ed = False, ing = False, n = False, er = False):\n",
    "    \n",
    "    raw = return_url_contents(url)\n",
    "    raw_lower = re.findall(r'\\b[a-z]+\\b', raw)\n",
    "    \n",
    "    unknown = sorted(set([w for w in raw_lower if w not in wordlist]))\n",
    "    \n",
    "    exclude = []\n",
    "\n",
    "    # words with -es plurals\n",
    "    if es:\n",
    "        es = [i for i in unknown if i[-2:] == 'es' and i[:-2] in wordlist]\n",
    "        exclude += es\n",
    "        \n",
    "        ies = [i for i in unknown if i[-3:] == 'ies' and i[:-3] in wordlist]\n",
    "        exclude += ies\n",
    "    \n",
    "    # regular plurals\n",
    "    if s:\n",
    "        s = [i for i in unknown if i[-1] == 's' and i[:-1] in wordlist]\n",
    "        exclude += s\n",
    "        \n",
    "    # regular past tense forms\n",
    "    if ed:\n",
    "        # verbs with final -e\n",
    "        d = [i for i in unknown if i[-1:] == 'd' and i[:-1] in wordlist]\n",
    "        exclude += d\n",
    "        # regular verbs\n",
    "        ed = [i for i in unknown if i[-2:] == 'ed' and i[:-2] in wordlist]\n",
    "        exclude += ed\n",
    "        # verbs that double final consonant\n",
    "        dd = [i for i in unknown if i[-2:] == 'ed' and i[:-3] in wordlist]\n",
    "        exclude += dd\n",
    "        \n",
    "    # regular gerunds\n",
    "    if ing:\n",
    "        # verbs with final -e\n",
    "        ng = [i for i in unknown if i[-3:] == 'ing' and i[:-3] + 'e' in wordlist]\n",
    "        exclude += ng\n",
    "        # regular verbs\n",
    "        ing = [i for i in unknown if i[-3:] == 'ing' and i[:-3] in wordlist]\n",
    "        exclude += ing\n",
    "        # verbs that double final consonat\n",
    "        nng = [i for i in unknown if i[-3:] == 'ing' and i[:-4] in wordlist]\n",
    "        exclude += nng\n",
    "        \n",
    "    if n:\n",
    "        # negative contractions without final -'t\n",
    "        n = [i for i in unknown if i[-1:] == 'n' and i[:-1] in wordlist]\n",
    "        exclude += n\n",
    "        \n",
    "    if er:\n",
    "        # comparative forms\n",
    "        er = [i for i in unknown if i[-2:] == 'er' and i[:-2] in wordlist]\n",
    "        exclude += er\n",
    "        # comparative forms with final -y\n",
    "        ier = [i for i in unknown if i[-3:] == 'ier' and i[:-3] + 'y' in wordlist]\n",
    "        exclude += ier\n",
    "        # comparative forms with final -e\n",
    "        r = [i for i in unknown if i[-2:] == 'er' and i[:-1] in wordlist]\n",
    "        exclude += r\n",
    "        # superlative forms\n",
    "        est = [i for i in unknown if i[-3:] == 'est' and i[:-3] in wordlist]\n",
    "        exclude += est\n",
    "        # superlative forms with final -y\n",
    "        st = [i for i in unknown if i[-3:] == 'est' and i[:-2] in wordlist]\n",
    "        exclude += st\n",
    "        # superlative forms with final -e\n",
    "        iest = [i for i in unknown if i[-4:] == 'iest' and i[:-4] + 'y' in wordlist]\n",
    "        exclude += iest\n",
    "        \n",
    "    # return only those unknown words that have not been excluded\n",
    "    # by the above list comprehensions\n",
    "    return [i for i in unknown if i not in exclude]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0ab38227",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.bbc.com/news/world-us-canada-67673545'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "092024e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['antisemitism', 'apologised', 'appalled', 'appearing', 'asked', 'bugs', 'bystanders', 'called', 'calls', 'captures', 'claiming', 'comments', 'counterparts', 'creates', 'cries', 'criticised', 'critics', 'depicts', 'drugs', 'errors', 'extremes', 'filed', 'flamingos', 'has', 'heads', 'heats', 'hrs', 'implied', 'incidents', 'institutions', 'languages', 'loses', 'members', 'missions', 'oldest', 'planned', 'presidents', 'protests', 'pulled', 'punished', 'refused', 'released', 'remarks', 'rights', 'rules', 'says', 'sites', 'students', 'tendered', 'tenured', 'tries', 'trustees', 'twerking', 'universities', 'using']\n"
     ]
    }
   ],
   "source": [
    "print(unknown(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "02e1ccdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['antisemitism', 'apologised', 'cries', 'criticised', 'hrs', 'implied', 'twerking', 'universities']\n"
     ]
    }
   ],
   "source": [
    "print(unknown(url, True, True, True, True, True, True)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0a058",
   "metadata": {},
   "source": [
    "22. Examine the results of processing the URL http://news.bbc.co.uk/ using the regular\n",
    "expressions suggested above. You will see that there is still a fair amount of\n",
    "non-textual data there, particularly JavaScript commands. You may also find that\n",
    "sentence breaks have not been properly preserved. Define further regular expressions\n",
    "that improve the extraction of text from this web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8c44a7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Home - BBC News Homepage Accessibility links Skip to content Accessibility Help BBC Account Notifications Home News Sport Weather iPlayer Sounds Bitesize CBeebies CBBC Food Home News Sport Earth Reel '"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://news.bbc.co.uk/'\n",
    "return_url_contents(url)[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceda6527",
   "metadata": {},
   "source": [
    "23. Are you able to write a regular expression to tokenize text in such a way that the\n",
    "word don’t is tokenized into do and n’t? Explain why this regular expression won’t\n",
    "work: «n't|\\w+»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "04120f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do', \"n't\")]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"(.*?)(n't)|\\w+\", \"don't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f145c4c",
   "metadata": {},
   "source": [
    "24. Try to write code to convert text into hAck3r, using regular expressions and\n",
    "substitution, where e → 3, i → 1, o → 0, l → |, s → 5, . → 5w33t!, ate → 8. Normalize\n",
    "the text to lowercase before converting it. Add more substitutions of your own.\n",
    "Now try to map s to two different values: $ for word-initial s, and 5 for wordinternal\n",
    "s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e6143e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Hello suckers.  I ate your lunch.  It was delish.\"\n",
    "\n",
    "test = test.lower()\n",
    "\n",
    "org = ['ate', 'e', 'i', 'o', 'l', 's', '\\.']\n",
    "sub = ['8', '3', '1', '0', '|', '5', '5w33t!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6e220c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h3||0 5uck3r55w33t!  1 8 y0ur |unch5w33t!  1t wa5 d3|15h5w33t!'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(org)):\n",
    "    test = re.sub(org[i], sub[i], test)\n",
    "    \n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13524ce",
   "metadata": {},
   "source": [
    "25. Pig Latin is a simple transformation of English text. Each word of the text is\n",
    "converted as follows: move any consonant (or consonant cluster) that appears at\n",
    "the start of the word to the end, then append ay, e.g., string → ingstray, idle →\n",
    "idleay (see http://en.wikipedia.org/wiki/Pig_Latin).\n",
    "\n",
    "    a. Write a function to convert a word to Pig Latin.\n",
    "    \n",
    "    b. Write code that converts text, instead of individual words.\n",
    "    \n",
    "    c. Extend it further to preserve capitalization, to keep qu together (so that\n",
    "    quiet becomes ietquay, for example), and to detect when y is used as a consonant\n",
    "    (e.g., yellow) versus a vowel (e.g., style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "433911fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pig_latin(word):\n",
    "    word = re.sub(\"’\", \"'\", word) \n",
    "    \n",
    "    if not word.isalpha():\n",
    "        if \"'\" not in word:\n",
    "            return word\n",
    "    \n",
    "    caps = False\n",
    "    if word[0].isupper():\n",
    "        caps = True\n",
    "    word = word.lower()\n",
    "    \n",
    "    if word[0] in 'AEIOUaeiou':\n",
    "        pl = word + 'ay'\n",
    "    \n",
    "    elif len(word) == 1:\n",
    "        return word\n",
    "    \n",
    "    elif word[0] == 'y':\n",
    "        pl = word[1:] + 'yay'\n",
    "    \n",
    "    elif word[:2] == 'qu':\n",
    "        pl = word[2:] + 'quay'\n",
    "    \n",
    "    else:\n",
    "        start, end = re.findall(r'\\b^[^aeiouy]*|[aeiouy]{1}\\S*', word)\n",
    "        pl = end + start + 'ay'\n",
    "    \n",
    "    if caps == True:\n",
    "        pl = pl[0].upper() + pl[1:]\n",
    "    \n",
    "    return pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "63f14e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_punctuation(text, characters = [\"'\", '’', ')', ',', '.', ':', ';', '?', '!', ']', \"''\"]): \n",
    "    \n",
    "    text = iter(text)\n",
    "    current = next(text)\n",
    "    \n",
    "    for nxt in text:\n",
    "        if nxt in characters:\n",
    "            current += nxt\n",
    "        else:\n",
    "            yield current\n",
    "            current = nxt\n",
    "    \n",
    "    yield current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "092ee71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pig_latin_text(text):\n",
    "    pl = []\n",
    "    for t in re.findall(r'\\b[\\S]+\\b|[.,!?]', text):\n",
    "        pl.append(pig_latin(t))\n",
    "    return \" \".join(join_punctuation(pl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "06fbe06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"\"\"One time in sixth grade we were at recess and while I was running to \n",
    "my friends, I just so happened to kick a HUGE rock and without thinking I \n",
    "shouted at the top of my lungs MOTHERFUCKER And with my god-awful luck, my math\n",
    "teacher was sitting at the bench right BESIDE ME. He then took me inside to \n",
    "what I thought was yell at me but he just couldn’t stop laughing and sent \n",
    "me back outside with a literal candy bar. He is still my favorite teacher \n",
    "I've ever had.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "86ad4e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oneay imetay inay ixthsay adegray eway ereway atay ecessray anday ilewhay Iay asway unningray otay ymay iendsfray, Iay ustjay osay appenedhay otay ickkay aay Ugehay ockray anday ithoutway inkingthay Iay outedshay atay ethay optay ofay ymay ungslay Otherfuckermay Anday ithway ymay god-awful ucklay, ymay athmay eachertay asway ittingsay atay ethay enchbay ightray Esidebay Emay. Ehay enthay ooktay emay insideay otay atwhay Iay oughtthay asway ellyay atay emay utbay ehay ustjay ouldn'tcay opstay aughinglay anday entsay emay ackbay outsideay ithway aay iterallay andycay arbay. Ehay isay illstay ymay avoritefay eachertay I'veay everay adhay.\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pig_latin_text(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98052cde",
   "metadata": {},
   "source": [
    "27. Python’s random module includes a function choice() which randomly chooses\n",
    "an item from a sequence; e.g., choice(\"aehh \") will produce one of four possible\n",
    "characters, with the letter h being twice as frequent as the others. Write a generator\n",
    "expression that produces a sequence of 500 randomly chosen letters drawn from\n",
    "the string \"aehh \", and put this expression inside a call to the ''.join() function,\n",
    "to concatenate them into one long string. You should get a result that looks like\n",
    "uncontrolled sneezing or maniacal laughter: he haha ee heheeh eha. Use split()\n",
    "and join() again to normalize the whitespace in this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e317f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c757b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nxt = random.choice('aehh ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53998821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhhaha a heeheheaahaaheahhehaah aeaeah h h hehehe  hha  hhahahe aahhh hh hhh aeheh h ahehaeaa h  heeeeaaa he e a hehahh hhhe h eaehh hh hhaeh heehhe hahhaaahahh eehhea haeh  hhh hhahe h hah hae aa hah aahh ah hh ehhhahehahheah  ehhhahaaahhhhaehe  h e  eh aae a  e hhh  ee  ahahhhahh hhhhea eheae hhhahhahe ee h  he  ehhhe  hhaaa hehha  a hheh haaaehehhhh eeaehahaah hahhhhhhhheh aahaaeh h hhhae h aaah eahae hhheehhhahhhheaeaaahhahaeahaehhhheae e h ehehe aah a  e e eaheee aeeheh ahhhe ehhh h ehhe hh\n",
      "hhhaha a heeheheaahaaheahhehaah aeaeah h h hehehe hha hhahahe aahhh hh hhh aeheh h ahehaeaa h heeeeaaa he e a hehahh hhhe h eaehh hh hhaeh heehhe hahhaaahahh eehhea haeh hhh hhahe h hah hae aa hah aahh ah hh ehhhahehahheah ehhhahaaahhhhaehe h e eh aae a e hhh ee ahahhhahh hhhhea eheae hhhahhahe ee h he ehhhe hhaaa hehha a hheh haaaehehhhh eeaehahaah hahhhhhhhheh aahaaeh h hhhae h aaah eahae hhheehhhahhhheaeaaahhahaeahaehhhheae e h ehehe aah a e e eaheee aeeheh ahhhe ehhh h ehhe hh\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\n",
    "for i in range(500):\n",
    "    s += random.choice('aehh ')\n",
    "\n",
    "print(s)\n",
    "\n",
    "s = ' '.join(s.split())\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f879ba",
   "metadata": {},
   "source": [
    "29. Readability measures are used to score the reading difficulty of a text, for the\n",
    "purposes of selecting texts of appropriate difficulty for language learners. Let us\n",
    "define μw to be the average number of letters per word, and μs to be the average\n",
    "number of words per sentence, in a given text. The Automated Readability Index\n",
    "(ARI) of the text is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score\n",
    "for various sections of the Brown Corpus, including section f (popular lore) and\n",
    "j (learned). Make use of the fact that nltk.corpus.brown.words() produces a sequence\n",
    "of words, whereas nltk.corpus.brown.sents() produces a sequence of\n",
    "sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "571cf5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "817650f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They', 'neither', 'liked', 'nor', 'disliked', 'the', ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.words(categories = 'romance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a9d6e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dc61e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brown_ari(cat):\n",
    "    total_letters = 0\n",
    "    for w in brown.words(categories = cat):\n",
    "        total_letters += len(w)\n",
    "    \n",
    "    mu_w = total_letters / len(brown.words(categories = cat))\n",
    "    \n",
    "    mu_s = len(brown.words(categories = cat)) / len(brown.sents(categories = cat))\n",
    "    \n",
    "    return (4.71 * mu_w) + (0.5 * mu_s) - 21.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "712c508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Adventure\" is 4.0842.\n",
      "\"Belles Lettres\" is 10.9877.\n",
      "\"Editorial\" is 9.4710.\n",
      "\"Fiction\" is 4.9105.\n",
      "\"Government\" is 12.0843.\n",
      "\"Hobbies\" is 8.9224.\n",
      "\"Humor\" is 7.8878.\n",
      "\"Learned\" is 11.9260.\n",
      "\"Lore\" is 10.2548.\n",
      "\"Mystery\" is 3.8336.\n",
      "\"News\" is 10.1767.\n",
      "\"Religion\" is 10.2031.\n",
      "\"Reviews\" is 10.7697.\n",
      "\"Romance\" is 4.3492.\n",
      "\"Science Fiction\" is 4.9781.\n"
     ]
    }
   ],
   "source": [
    "for c in brown.categories():\n",
    "    ari = get_brown_ari(c)\n",
    "    c = re.sub('_', ' ', c)\n",
    "    print(\"\\\"{}\\\" is {:.4f}.\".format(c.title(), ari))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80141e90",
   "metadata": {},
   "source": [
    "30. Use the Porter Stemmer to normalize some tokenized text, calling the stemmer\n",
    "on each word. Do the same thing with the Lancaster Stemmer, and see if you observe\n",
    "any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20819309",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Martin_Porter'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2e39cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin Porter - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Donate Contribute Help Learn t\n"
     ]
    }
   ],
   "source": [
    "to_be_stemmed = return_url_contents(url)\n",
    "print(to_be_stemmed[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95657b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Martin', 'Porter', '-', 'Wikipedia', 'Jump', 'to', 'content', 'Main', 'menu', 'Main', 'menu', 'move', 'to', 'sidebar', 'hide', 'Navigation', 'Main', 'page', 'Contents', 'Current', 'events', 'Random', 'article', 'About', 'Wikipedia', 'Contact', 'us', 'Donate', 'Contribute', 'Help', 'Learn', 'to', 'edit', 'Community', 'portal', 'Recent', 'changes', 'Upload', 'file', 'Languages', 'Language', 'links', 'are', 'at', 'the', 'top', 'of', 'the', 'page', 'across']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(to_be_stemmed)\n",
    "print(tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09bc3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b356b7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['martin', 'porter', '-', 'wikipedia', 'jump', 'to', 'content', 'main', 'menu', 'main', 'menu', 'move', 'to', 'sidebar', 'hide', 'navig', 'main', 'page', 'content', 'current', 'event', 'random', 'articl', 'about', 'wikipedia', 'contact', 'us', 'donat', 'contribut', 'help', 'learn', 'to', 'edit', 'commun', 'portal', 'recent', 'chang', 'upload', 'file', 'languag', 'languag', 'link', 'are', 'at', 'the', 'top', 'of', 'the', 'page', 'across']\n"
     ]
    }
   ],
   "source": [
    "print([porter.stem(w) for w in tokens][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09a18a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['martin', 'port', '-', 'wikiped', 'jump', 'to', 'cont', 'main', 'menu', 'main', 'menu', 'mov', 'to', 'sideb', 'hid', 'navig', 'main', 'pag', 'cont', 'cur', 'ev', 'random', 'artic', 'about', 'wikiped', 'contact', 'us', 'don', 'contribut', 'help', 'learn', 'to', 'edit', 'commun', 'port', 'rec', 'chang', 'upload', 'fil', 'langu', 'langu', 'link', 'ar', 'at', 'the', 'top', 'of', 'the', 'pag', 'across']\n"
     ]
    }
   ],
   "source": [
    "print([lancaster.stem(w) for w in tokens][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a39ce",
   "metadata": {},
   "source": [
    "31. Define the variable saying to contain the list ['After', 'all', 'is', 'said',\n",
    "'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']. Process the  list\n",
    "using a for loop, and store the result in a new list lengths. Hint: begin by assigning\n",
    "the empty list to lengths, using lengths = []. Then each time through the loop,\n",
    "use append() to add another length value to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0822a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "saying = ['After', 'all', 'is', 'said',\n",
    "'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "183577f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "\n",
    "for w in saying:\n",
    "    length.append(len(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a210852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9c156f",
   "metadata": {},
   "source": [
    "32.Define a variable silly to contain the string: 'newly formed bland ideas are\n",
    "inexpressible in an infuriating way'. (This happens to be the legitimate interpretation\n",
    "that bilingual English-Spanish speakers can assign to Chomsky’s famous\n",
    "nonsense phrase colorless green ideas sleep furiously, according to Wikipedia). Now\n",
    "write code to perform the following tasks:\n",
    "\n",
    "a. Split silly into a list of strings, one per word, using Python’s split() operation,\n",
    "and save this to a variable called bland.\n",
    "\n",
    "b. Extract the second letter of each word in silly and join them into a string, to\n",
    "get 'eoldrnnnna'.\n",
    "\n",
    "c. Combine the words in bland back into a single string, using join(). Make sure\n",
    "the words in the resulting string are separated with whitespace.\n",
    "\n",
    "d. Print the words of silly in alphabetical order, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30624276",
   "metadata": {},
   "outputs": [],
   "source": [
    "silly = \"newly formed bland ideas are inexpressible in an infuriating way\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7039a9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible', 'in', 'an', 'infuriating', 'way']\n"
     ]
    }
   ],
   "source": [
    "bland = silly.split()\n",
    "print(bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34548af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eoldrnnnna'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seconds = \"\"\n",
    "for i in bland:\n",
    "    seconds += i[1]\n",
    "    \n",
    "seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca6a004a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newly formed bland ideas are inexpressible in an infuriating way'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a406612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'are', 'bland', 'formed', 'ideas', 'in', 'inexpressible', 'infuriating', 'newly', 'way']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(bland))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dbc44a",
   "metadata": {},
   "source": [
    "33. The index() function can be used to look up items in sequences. For example,\n",
    "'inexpressible'.index('e') tells us the index of the first position of the letter e.\n",
    "\n",
    "a. What happens when you look up a substring, e.g., 'inexpressi\n",
    "ble'.index('re')?\n",
    "\n",
    "b. Define a variable words containing a list of words. Now use words.index() to\n",
    "look up the position of an individual word.\n",
    "\n",
    "c. Define a variable silly as in Exercise 32. Use the index() function in combination\n",
    "with list slicing to build a list phrase consisting of all the words up to\n",
    "(but not including) in in silly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33b43e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'inexpressible'.index('re')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3081ca27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"I'm\", 'too', 'tired', 'think', 'of', 'a', 'more', \n",
    "         'original', 'list']\n",
    "\n",
    "words.index('tired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6c521d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in silly.split()][:silly.split().index(\"in\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db9ec0d",
   "metadata": {},
   "source": [
    "34. Write code to convert nationality adjectives such as Canadian and Australian to\n",
    "their corresponding nouns Canada and Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff5a47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8e0eac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/STRZGR/Natural-Language-Processing-with-Python-Analyzing-Text-with-the-Natural-Language-Toolkit/master/Chapter%2003/Nationalities.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7aad3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "nationalities = pd.read_csv(url, encoding='windows-1254', header = None, names = [\"n\", 'adj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4cab6812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>adj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abkhazia</td>\n",
       "      <td>Abkhaz, Abkhazian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Afghan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Åland Islands</td>\n",
       "      <td>Åland Island</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Albanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Algerian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               n                adj\n",
       "0       Abkhazia  Abkhaz, Abkhazian\n",
       "1    Afghanistan             Afghan\n",
       "2  Åland Islands       Åland Island\n",
       "3        Albania           Albanian\n",
       "4        Algeria           Algerian"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nationalities[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa288d",
   "metadata": {},
   "source": [
    "35. Read the LanguageLog post on phrases of the form as best as p can and as best p\n",
    "can, where p is a pronoun. Investigate this phenomenon with the help of a corpus\n",
    "and the findall() method for searching tokenized text described in Section 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7178c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "72c573b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bw = nltk.Text(brown.words())\n",
    "\n",
    "bw.findall(r\"<as> <best> <as> <.*> <can>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5cfb06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bw.findall(r\"<as> <best> <.*> <can>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a13abc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = [\"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\"]\n",
    "\n",
    "phrases = [\"as best as X can\", \"as best X can\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f43ffd0",
   "metadata": {},
   "source": [
    "37.Read about the re.sub() function for string substitution using regular expressions,\n",
    "using help(re.sub) and by consulting the further readings for this chapter.\n",
    "Use re.sub in writing code to remove HTML tags from an HTML file, and to\n",
    "normalize whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34fcf51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"\n",
    "<HTML>\n",
    "\n",
    "<HEAD>\n",
    "\n",
    "<TITLE>Your Title Here</TITLE>\n",
    "\n",
    "</HEAD>\n",
    "\n",
    "<BODY BGCOLOR=\"FFFFFF\">\n",
    "\n",
    "<CENTER><IMG SRC=\"clouds.jpg\" ALIGN=\"BOTTOM\"> </CENTER>\n",
    "\n",
    "<HR>\n",
    "\n",
    "<a href=\"http://somegreatsite.com\">Link Name</a>\n",
    "\n",
    "is a link to another nifty site\n",
    "\n",
    "<H1>This is a Header</H1>\n",
    "\n",
    "<H2>This is a Medium Header</H2>\n",
    "\n",
    "Send me mail at <a href=\"mailto:support@yourcompany.com\">\n",
    "\n",
    "support@yourcompany.com</a>.\n",
    "\n",
    "<P> This is a new paragraph!\n",
    "\n",
    "<P> <B>This is a new paragraph!</B>\n",
    "\n",
    "<BR> <B><I>This is a new sentence without a paragraph break, in bold italics.</I></B>\n",
    "\n",
    "<HR>\n",
    "\n",
    "</BODY>\n",
    "\n",
    "</HTML>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a9ec0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting rid of tags\n",
    "\n",
    "example = re.sub(r\"<.*?>\", '', example)\n",
    "\n",
    "# normalizing whitespace\n",
    "\n",
    "example = re.sub(r\"\\s+\", ' ', example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bba96585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Your Title Here Link Name is a link to another nifty site This is a Header This is a Medium Header Send me mail at support@yourcompany.com. This is a new paragraph! This is a new paragraph! This is a new sentence without a paragraph break, in bold italics. '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9774fa",
   "metadata": {},
   "source": [
    "38.An interesting challenge for tokenization is words that have been split across a\n",
    "linebreak. E.g., if long-term is split, then we have the string long-\\nterm.\n",
    "\n",
    "a. Write a regular expression that identifies words that are hyphenated at a linebreak.\n",
    "The expression will need to include the \\n character.\n",
    "\n",
    "b. Use re.sub() to remove the \\n character from these words.\n",
    "\n",
    "c. How might you identify words that should not remain hyphenated once the\n",
    "newline is removed, e.g., 'encyclo-\\npedia'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b0b45ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_line_break(text, wordlist):\n",
    "    if '-' in text:\n",
    "        hypen = re.findall(r\"\\b\\w*-\\n\\w*\\b\", text)\n",
    "        for h in hypen:\n",
    "            check = re.sub(r\"-\\n\", \"-\", h)\n",
    "            if check in wordlist:\n",
    "                text = re.sub(h, check, text)\n",
    "            else:\n",
    "                text = re.sub(h, re.sub(r'-\\n', '', h), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "611da450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm trying to find the long-term solution.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_words = sorted(set([w.lower() for w in nltk.corpus.brown.words()]))\n",
    "\n",
    "test = \"I'm trying to find the long-\\nterm solution.\"\n",
    "\n",
    "check_line_break(test, brown_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2894f48b",
   "metadata": {},
   "source": [
    "40. Obtain raw texts from two or more genres and compute their respective reading\n",
    "difficulty scores as in the earlier exercise on reading difficulty. E.g., compare ABC\n",
    "Rural News and ABC Science News (nltk.corpus.abc). Use Punkt to perform sentence\n",
    "segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "871ac990",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = nltk.corpus.abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73ee3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ari(corpus, cat):\n",
    "    \n",
    "    total_letters = 0\n",
    "    for w in corpus.words(fileids = cat):\n",
    "        total_letters += len(w)\n",
    "        \n",
    "    # calculate average number of letters per word\n",
    "    mu_w = total_letters/len(corpus.words(fileids = cat))\n",
    "    \n",
    "    # calculate average number of words per sentence\n",
    "    mu_s = len(corpus.words(fileids = cat)) / len(corpus.sents(fileids = cat))\n",
    "    \n",
    "    return (4.71 * mu_w) + (0.5 * mu_s) - 21.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d0194cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"rural\": 12.3487.\n",
      "\"science\": 12.5276.\n"
     ]
    }
   ],
   "source": [
    "for f in abc.fileids():\n",
    "    ari = get_ari(abc, f)\n",
    "    f = re.sub(\".txt\", \"\", f)\n",
    "    print(\"\\\"{}\\\": {:.4f}.\".format(f, ari))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008f443",
   "metadata": {},
   "source": [
    "41. Rewrite the following nested loop as a nested list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "434f00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    " words =['attribution', 'confabulation', 'elocution', 'sequoia', 'tenacious', 'unidirectional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "336ad844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'oauaio', 'eouio', 'euoia', 'eaiou', 'uiieioa']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[''.join([ch for ch in w if ch.lower() in 'aeiou']) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eceb7b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "767abcb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2084071"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')[0].offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebc56e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8029939f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('dog.n.01')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "214472ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')[0].pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ffe9d",
   "metadata": {},
   "source": [
    "43. With the help of a multilingual corpus such as the Universal Declaration of\n",
    "Human Rights Corpus (nltk.corpus.udhr), along with NLTK’s frequency distribution\n",
    "and rank correlation functionality (nltk.FreqDist, nltk.spearman_correla\n",
    "tion), develop a system that guesses the language of a previously unseen text. For\n",
    "simplicity, work with a single character encoding and just a few languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5030fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import udhr\n",
    "from nltk.metrics.spearman import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b166924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_language(text, encoding = '-Latin1'):\n",
    "    if type(text) != list:\n",
    "        text = word_tokenize(text)\n",
    "    \n",
    "    fd = nltk.FreqDist([ch for ch in [ch for word in text for ch in word.lower() if word.isalpha()]])\n",
    "    ranked_letters =[l for r, l in sorted([(v, k) for k, v in fd.items()], reverse = True)]\n",
    "    \n",
    "    languages = [l for l in nltk.corpus.udhr.fileids() if l.endswith(encoding)]\n",
    "    \n",
    "    max_corr = float(\"-inf\")\n",
    "    top_cand = \"\"\n",
    "    \n",
    "    # loop through all the languages and find the one with the highest spearman correlation\n",
    "    for lang in languages:\n",
    "        fd = nltk.FreqDist([ch for ch in [ch for word in udhr.words(lang) for ch in word.lower() if word.isalpha()]])\n",
    "        fd_letters = [l for r, l in sorted([(v, k) for k, v in fd.items()], reverse = True)]\n",
    "        sc = spearman_correlation(ranks_from_sequence(ranked_letters), \n",
    "                               ranks_from_sequence(fd_letters))\n",
    "        if sc > max_corr:\n",
    "            top_cand, max_corr =  lang, sc\n",
    "\n",
    "    # replace underscore with hyphen\n",
    "    top_cand = re.sub('_', '-', top_cand)\n",
    "    \n",
    "    # remove encoding from language name\n",
    "    top_cand = re.sub(encoding, '', top_cand)\n",
    "    \n",
    "    print(\"The most likely language is \" + top_cand + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be076b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.tagesschau.de/ausland/brexit-1083.html'\n",
    "\n",
    "de = return_url_contents(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9d00b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most likely language is German-Deutsch.\n"
     ]
    }
   ],
   "source": [
    "guess_language(de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f00e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.rtp.pt/noticias/lusa/brexit-boris-johnson-suspende-legislacao-do-acordo-apos-derrota-parlamentar_n1180932'\n",
    "pt = return_url_contents(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "093f580e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most likely language is Galician-Galego.\n"
     ]
    }
   ],
   "source": [
    "guess_language(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fcdd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
